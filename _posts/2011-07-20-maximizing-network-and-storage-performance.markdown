--- 
title: Maximizing Network and Storage Performance
type: post
layout: post
tags: 
- Big Data
- Storage
- !binary |
  5aSn6K+d5a2Y5YKo

---
<p>2011-07-19 2:00——5:00</p> <p><strong>Maximizing Network and Storage Performance for Big Data Analytics</strong></p> <p>+Digital Data Explosion inHuman Society</p> <blockquote> <p>Analog Storage</p> <p>Digital Storage</p></blockquote> <p>+Challenge of Big Data Management and Analytics</p> <blockquote> <p>Existing DB Tech is not prepared for the huge volume</p> <p>. 2007,facebook had a 15TB data warehouse by a big-DBMS-wendor</p> <p>. 70TB added into facebook data/day</p> <p>.Commercial parallel DBs rarely have 100+ nodes</p> <p>. Yahoo hadoop cluster has 4000+nodes,facebook 2750+ nodes</p> <p><strong>Challenge</strong></p> <p>++ 1.Big data is about all kinds of data</p> <blockquote> <p>. click-stream</p> <p>. medical image analytics r crucial to research and diagnosis</p></blockquote> <p>++ 2. Complex analytics to gain deep insights from big data</p> <p>++ 3. Conventional database business models is not affordable</p> <blockquote> <p>. license</p> <p>. hing maintenance fees</p> <p>. $10000/TB</p> <p>. Hadoop-like systems cost $1500/TB</p></blockquote> <p>++ 4. Conventional database processing model is “scale-up” based</p> <blockquote> <p>Big-data processing model is “scale-out”</p></blockquote></blockquote> <p><strong>MapReduce programming model become efficent processing engine to solve big data.</strong></p> <p>+ Why MapReduce</p> <blockquote> <p>. to process huge volumes of data concurrently</p> <p>. two unique properties:</p> <p>  .. minimum dependency among tasks/sharing nothing</p> <p>  .. simple task operations in each node—>容错</p> <p>. Two strong merits for big data anaytics</p> <p>  .. scalability: increasing nodes</p> <p>  .. Fault-tolerance</p></blockquote> <p>Hadoop is the most widely used implementation of MapReduce.</p> <blockquote> <p>eg:AOL,baidu,ebay,facebook,IBM,NY Times,Yahoo!…</p></blockquote> <p>+MapReduce Overview</p> <blockquote> <p>.Map: (k1,v1)—>(k2,v2)</p> <p>.Reduce: (k2,v2)—>(k3,v3)</p> <p>.Shuffle: Partition Key(the same k2,or not)</p></blockquote> <p>+Example of MR job on Hadoop</p> <blockquote> <p>{name:(dept,salary)}—>{dept:(avg_salary)}</p> <p>  map:{name:(dept,salary)}—>{dept:salary}</p> <p>  shuffle: put same dept into one set.</p> <p>  reduce:{dept:salary}—>{dept:avg_salary}</p></blockquote> <p>+ MR Job Excution Patterns</p> <blockquote> <p>. Master node</p> <p>  .. control level work,scheduling and task assignment</p> <p>. Data is stored in DFS(eg.HDFS)</p> <p><strong>STEP (and cost):</strong></p> <p>  a. Job submissin (IO)</p> <p>  b. worker nodes</p> <p>  c. Map (local IO)</p> <p>  d. Shuffle (network costs)</p> <p>  e. Reduce</p> <p>  f. store output into DFS (local IO and network)</p></blockquote> <p> </p> <p><strong>//下面的两个挑战，是重点，解决方式见《<a href="http://u.ownlinux.net/nourl/2011/rcfile.html" target="_blank">RCFile</a>》及《<a href="http://u.ownlinux.net/nourl/2011/ysmart.html" target="_blank">Ysmart</a>》两篇文章</strong></p> <p><strong>+ Two Critical Challenges in Production Systems</strong></p> <blockquote> <p>. Background:standard Relational DB have moved to MR Environment,such as Hive and Pig by fb and Yahoo!</p> <blockquote> <p><strong>+challenge1. how to initially store the data in DS</strong></p> <p>  .. network and storage cost </p> <p>STEP:</p> <p align="left"> <strong>DATA loading (L)</strong></p> <p align="left"><strong>QUERY processing (P)</strong></p> <p align="left"><strong>STORAGE spcace utilization (S)</strong></p> <p align="left"><strong>ADAPTIVITY to dynamic workload patterns (W)</strong></p> <p><strong>+challenge2. How to auto convert RDB queries into MR jobs</strong></p> <p>  .. cost</p> <p> <strong> SQL-to-MR Translator: </strong>a interface between users and MR programs.  eg. Hive / Pig</p> <p>    95+% in facebook by Hive</p> <p>    75/+% in yahoo!   by Pig</p></blockquote> <p>Addressing these two challenges, we maximize:</p> <p>  .. performance of big data analytics</p> <p>  .. productiveity of …………….</p></blockquote>

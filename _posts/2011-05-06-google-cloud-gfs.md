---
title:  google cloud gfs
type: post
layout: post
tags: 
- Cloud
- GFS
- Google
---
<h2> </h2><h3>1. why</h3><blockquote>Google为什么要开发GFS？新的工程总是为了满足新的需求来解决一定的问题，Google也不例外。<br />Google面临的问题及特殊需求就是:<br />a. 要存储及处理海量的内容<br />b. 要对用户的搜索作出实时的相应<br />因此，Google开发出了GFS，用Google工程师的话说，就是 “GFS is designed for Google Apps,and Google Apps are designed for GFS”.<br />Google设计GFS是为了满足自己的App的需要，当然，当Google再设计提供App时，会考虑使用已有的GFS.</blockquote><h3>2. Motivation</h3><blockquote>设计GFS时，Google的想法就是将大量数据存储在廉价的不可靠的计算机上，通过使用数据副本的方式来提供满足需要的分布式文件系统.</blockquote><h1></h1><h3>3. Google File System</h3><blockquote>Google的问题有一定的特殊性，这也是使用GFS的特殊条件:</blockquote><blockquote>a. 系统内的组件发成错误是常态 <br />b. 存储的大文件没有达到不可接受的程度，大部分文件都是MB或者GB级别 <br />c. 数据只写一次，读多次（可能是并发读数据） <br />d. 在低延迟的情况下，提供高的处理能力 </blockquote><blockquote>下面重点看一下GFS的架构及节点的功能：</blockquote><img src="http://farm6.static.flickr.com/5107/5690214596_6a93f752c8.jpg" /> <br /><div align="center"> 系统架构</div><div style="text-align: left;">GFS中主要存在三种节点，Master、Chunk及Client</div><div style="text-align: left;">I) Master</div><blockquote>Google 使用简单集中式管理，使用单个Master<br />当然，使用单个master存在单点失败（single point of failure）的问题，当chunk数量很大的时候，使用单个master会成为整个系统的瓶颈. <br />Google提出的解决方案就是，一是使用Shadow master作为备份，当master失败了，立即启用Shadow Master接管工作（这里面存在Master与shadow master同步的问题），二是最小化与master相关的数据及操作，减轻它的负担. <br />例如数据流不经过master，它仅仅存储chunk的元数据，在client及metadata之间添加cache，减少对metadata的反复请求等。<br /><b>Master的功能</b>：<br />a. 管理chunk节点，send指令，get状态<br />b. 存储chunk的命名空间，保存chunk的元数据<br />c. 记录chunk存储的内容及位置，chunk副本的位置<br />d. 垃圾回收：将删除操作写入日志，rename要删除的文件为指定的格式，仅必须时才回收这些特殊格式的文件. <br />e. 根据chunk的版本号，删除陈旧无用的副本<br />f. 管理文件系统：对于client请求的文件，传入文件的key值，返回的是该文件所在chunk的index num<br />g. 负责整个chunk存储系统的负载均衡问题 <br /><br />这里提到了元数据，元数据中主要存储三种数据：<br />a. Chunk的namespaces及文件的name<br />b. 文件与chunk的map关系<br />c. 每个chunk副本的位置 </blockquote>II) Chunk<br /><blockquote>  实现存储功能，以64M为单位chunk进行存储，chunk与index num一一对应，chunk的加入和离开需要与master沟通（注册与删除）<br />  每个chunk都有internal buffers，用于暂存client写入的数据<br />  读取的过程:<br />a. Client发送request到master      <br />b. master返回该文件所在chunk的location       <br />c. client发送request到chunk，读文件       <br />d. chunk返回data到client<br />  写入的过程 :<br />a. client发送写请求到master      <br />b. master返回chunk handle及副本的location       <br />c. client写数据到chunk及其副本的buffer中       <br />d. Primary chunk分配data在chunk中所处的位置，写数据进入chunk       <br />e. 保存该数据副本的chunk同步数据，在chunk中写入数据       <br />f. 副本chunk返回写入状态至primary chunk       <br />g. primary chunk返回写入状态至client </blockquote>III) Client<br /><blockquote>实现了一个库，提供访问GFS的API函数，调用库函数可以访问数据.<br />另外，除了这三种节点之外，GFS中还需要实现以下两个主要模块:<br />a. 监控模块<br />  GFS为了持久的正常工作，还需要实现一个监控模块，对整个系统中出现的故障进行自动检测。<br />b. 日志模块<br />  为了记录操作及恢复数据，GFS中需要一个日志模块.</blockquote> <b>GFS特点</b><br /><blockquote><b>  </b>通过分析GFS的架构，可以发现在GFS中，控制流存在与client和master之间，数据流存在与client和chunk之间，控制流和数据流是分开的。<br />  在GFS中，在必要的地方使用了缓存机制，比如当client请求master中的元数据时，在读chunk的过程中，并没有使用缓存（为什么没有？让我再想一想…..<span style="font-size: xx-small;"><i>必要性+可行性</i></span>）。<br />  GFS是在用户态实现，这种实现方式考虑到GFS自身的与Kernel的单独升级。<br />  在用户太实现GFS，我觉得满足了Gogole的特殊需求，并且Google有这个技术这样实现，也方便了GFS的更新和Kernel的升级，但是： <br />a. 用户态编程调试的难度不应该成为限制文件系统由内核态实现的理由      <br />b. Posix接口可以提供强大的功能不应该成为理由，直接驱动硬盘等更高效       <br />c. 用户态实现方便了chunk之间及chunk与master的隔离，用户进程进程的隔离和内核线程应该都可以实现相同的功能.       <br />d. GFS没有实现与POSIX兼容的接口，通用的数据中心应该提供标准的接口，供不同的应用、用户满足不同的需求.</blockquote><b>一致性问题</b><br /><blockquote>  由于每个chunk中的数据存在多个副本（默认是三个）以保证系统的可靠性，数据之间的一致性Consistency问题特别突出</blockquote><b>容错问题 </b><br /><blockquote>GFS中，由文件系统实现容错：<br />a. Master:备份+日志文件+Master备份节点 <br />b. Chunk Server:每个chunk都有副本 <br />c. GFS中的每个节点都应该可以快速重启恢复工作 </blockquote><b>可靠性问题</b><br /><blockquote>用软件的方法实现可靠性</blockquote><br /><span style="color: navy;">GFS不一定当前是最好的分布式存储系统，但它使用简单、低成本的方式解决了Google的难题，对Google来讲，它就是最好的！</span><br /><span style="color: navy;"></span>

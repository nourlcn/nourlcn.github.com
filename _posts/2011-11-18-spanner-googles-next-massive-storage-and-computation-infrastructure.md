--- 
title: "Spanner: Google\xE2\x80\x99s next Massive Storage and Computation infrastructure"
type: post
layout: post
tags: 
- Google
- Spanner
---
http://www.royans.net/arch/spanner-googles-next-massive-storage-and-computation-infrastructure/ <br /><br /><a href="http://www.royans.net/arch/category/mapreduce/">MapReduce</a>, <a href="http://en.wikipedia.org/wiki/BigTable">Bigtable</a> and <a href="http://www.royans.net/arch/pregel-googles-other-data-processing-infrastructure/">Pregel</a>  have their origins in Google and they all deal with “large systems”.  But all of them may be dwarfed in size and complexity by a new project  Google is working on, which was mentioned briefly (may be  un-intentionally) at an event last year. <img height="64" src="http://lh4.ggpht.com/_7ZYqYi4xigk/SphYLmmZU5I/AAAAAAAAESM/yD9mxtlMHk4/s288/google_full.jpg" style="float: right;" width="177" /><br />Instead  of caching data closer to user, it looks like Google is trying to take  “the data” to the user. If you use GMail or a Google Doc service, then  with this framework, Google could, auto-magically, “move” one of the  master copies of your data to the nearest Google data center without  really having to cache anything locally. And because they are building  one single datastore cluster around the world, instead of building  hundreds of smaller ones for different applications, it looks like they  may not don’t need dedicated clusters for specific projects anymore.<br />Below is the gist of “Spanner” from a talk by Jeff Dean at <a href="http://www.cs.cornell.edu/projects/ladis2009/talks/dean-keynote-ladis2009.pdf">Symposium held at Cornell</a>.  Take a look at the rest of the slides if you are interested in some  impressive statistics on hardware performance and reliability.<br /><ul><li><em><strong>Spanner</strong>: Storage & computation system that spans all our datacenters </em><ul><li><em>Single global namespace</em><ul><li><em>Names are independent of location(s) of data</em></li><li><em>Similarities to Bigtable: table, families, locality groups, coprocessors,…</em></li><li><em>Differences: hierarchical directories instead of rows, fine-grained replication</em></li><li><em>Fine-grained ACLs, replication configuration at the per-directory level</em></li></ul></li><li><em>support mix of strong and weak consistency across datacenters</em><ul><li><em>Strong consistency implemented with </em><a href="http://en.wikipedia.org/wiki/Paxos_algorithm"><em>Paxos</em></a><em> across tablet replicas</em></li><li><em>Full support for distributed transactions across directories/machines</em></li></ul></li><li><em>much more automated operation</em><ul><li><em>System automatically moves and adds replicas of data and computation based on constraints and usage patterns</em></li><li><em>Automated allocation of resources across entire fleet of machines.</em></li></ul></li></ul></li></ul>
